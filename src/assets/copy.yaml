name: "A Modern Scroll"
intro:
  lesson_num: "Lesson 1"
  lesson_title: "The K-Nearest Neighbors Algorithm, and Rookies
  <br>
  in the NBA."
  preamble1: "Pre-reqs:"
  goals1:
    - "An interest in machine learning."
    - "12 minutes of free time."
  preamble2: "This lesson:"
  goals2:
    - "I'll introduce you to a fun dataset about rookie (1st year) NBA players."
    - "We'll use the K-Nearest Neighbors (KNN) algorithm to predict which rookies will have NBA careers that last 5 years or more."
    - "You will tune our KNN model's <span class='tooltip'>hyperparameters<span class='tooltiptext'>This is just a fancy word for a model's settings.</span></span>."
    - "We'll measure how accurately the model predicts which rookies will have lasting NBA careers."
part1:
  step1:
    heading: "The Dataset"
    texta:
      "For this lesson, we’ll use <span class='tooltip'>this awesome dataset<a class='tooltiptext' href='google.com'>Link to dataset</a></span>
      about 1st-year NBA players, aka <em>rookies</em>.
      <br><br>
      Some rookies will drop out of the NBA pretty soon after starting; others
      will stay in the NBA for a long time. Today we're going to use this dataset
      to make a simple model that will let us predict which rookie players will
      stick around in the NBA for at least 5 years. Our model will be able to
      accurately make this prediction about a player's future after they're done
      playing their rookie year in the NBA. This is a whole lot like a weatherman
      predicting the weather; only our predictions will be about whether or not
      a rookie NBA'er will have a basketball career for at least 5 years.
      <br><br>
      "
    textb:
      "Take a moment to explore the dataset. Mouse over column names for more info."
  step2:
    heading: "Classification"
    texta:
      "A common task in machine learning is to train a computer to accurately
      guess whether a new mystery datapoint belongs in one group (also called a
      <em>class</em>) or another. This is called a <em>classification task</em>.
      <br><br>
      In our dataset, each row represents one rookie NBA player. The <em>class</em>
      of each player is whether or not they lasted 5 years in the NBA.
      "
    textb:
      "Let's train a simple model that takes two datapoints about a player as
      inputs: (1) the player's average number of shots made, and (2) their
      average number of rebounds, per game, in their rookie year.
      <br><br>
      And we'll make our model give us one output: a guess about whether the
      rookie player belongs in the 'Under 5 years' class (players who will
      have a career that lasts under 5 years) or the '5+ years' class."
  step3:
    heading: "Mystery Players"
    texta:
      "Let's say you and I have a favorite rookie NBA player named
      <span class='tooltip'>Taj Gibson<a class='tooltiptext' href='google.com'>
      Yes, I know Taj was drafted a decade ago. We're pretending!</a></span>.
      It's the end of Taj's rookie year in the NBA, and we want to know whether
      or not he's going to stick around for 5+ amazing years.
      <br><br>
      We know that Taj has made an average of 3.8 shots per game, and had a solid
      average of 4.7 rebounds per game."
    textb:
      "How could we train a computer to predict whether or not Taj's NBA career
      will last at least 5 years?"
  step4:
    heading: "Classifying Taj Gibson"
    texta:
      "The task above is asking us to train a computer to determine the class of a
      new row. This kind of task is called a classification task. We will solve it
      by using a classification algorithm called K-Nearest Neighbors."
    textb:
      "But first, let’s take a step back and graph this data. We’ll create a
      scatterplot, with a horizontal axis representing the average number of shots
      that players made per game, and a vertical axis representing their average
      rebounds per game. Each dot will represent one player."
    textc:
      "Blue dots represent players whose careers lasted 5+ years, and yellow dots
      are the players whose careers lasted less than 5 years. If you forget any of
      that, just mouse over any datapoint for details.
      <br><br>
      Let's also include a few more datapoints from the same dataset, to make this
      more interesting."

part2:
  step1:
    heading:
      "A Scatterplot of NBA'ers"
    texta:
      "Holy dog, a scatterplot! Now that we have the data plotted, let’s
      talk about how we'll use the K-Nearest Neighbors algorithm to solve
      our classification task."
  step2:
    heading: "Taj Gibson, Our Mystery Player"
    texta:
      "Here's Taj, blinking on our scatterplot. He's made an average of
      3.8 shots, and 4.7 rebounds, per game. But we don't know his class--
      will he last 5+ years in the NBA, or not?"
    textb:
      "The K-Nearest Neighbors algorithm will look at the nearest neighbors of
      this new datapoint, and will tally up how many of them are '5+ Year Career'
      players, and how many are 'Under 5 year career' players."
  step3:
    heading: "K Neighbors"
    texta: "How many neighbors will KNN look at?"
    textb:
      "That’s what K stands for! It’s the number of nearest neighbors to tally up!
      Let’s say that K is 5 for now."
    textc:
      "How does KNN know which datapoints are the nearest neighbors?"
    textd:
      "It calculates the
      <span class='tooltip'>
        distance
        <span class='tooltiptext'>
        A common way to calculate distance is using
          <a href='https://en.wikipedia.org/wiki/Euclidean_distance' target='_blank'>
            the euclidean distance formula
          </a> you might remember from high school algebra.
        </span>
      </span> from the new datapoint, (at location $XXX k, 5)
      to every other datapoint on the grid. Then it chooses the top 5."
  step4:
    heading: "Majority Wins"
    texta:
      "Then it will take the majority vote of those 5 datapoints, and that is the
      algorithm’s guess about what the mysterious new datapoint is."
    textb:
      "In this case, 4/5 of the nearest neighbors are apartments— so our best
      guess is that this new datapoint is an apartment."
  step5:
    heading: "What Happens if we Change K?"
    texta:
      "K is the model hyper-parameter that determines how many neighbors to take
      into account. We get to pick what K is. With two possible classes in our
      dataset, we can choose K to be any odd number, from 1 up to the size of the
      entire dataset."
    textb:
      "Use the slider below to play with the value for K. Notice how it affects
      the model’s guess about the class of the mystery datapoint.
      This is called tuning the model parameters, and is a big part of creating
      accurate machine learning models."
  step6:
    heading: "What a Simple Model!"
    texta:
      "We'll dive deeper into model parameter tuning in future lessons.
      For now, let’s just assume that we chose to use K = 5 for our model."
    textb:
      "When you first learn about KNN, it might seem wild that this could
      possibly work. It seems too basic, too reductive."
    textc:
      "How well does the model actually work? And how do we measure that?"
  step7:
    heading: "Testing our Model's Accuracy"
    texta:
      "Let’s remove the labels (house/apartment) from 100 of our 500 datapoints,
      and run the algorithm on each of those 100 “mystery” datapoints. We know
      what their true labels are supposed to be, and we have KNN’s guesses about
      what the labels should be. So, we can count how many it got right."
    textb:
      "After doing this, we find that it guesses correctly on 75% of the
      datapoints! You would expect it to get 50% of them correct just by chance,
      so it’s way better than chance. This metric is called accuracy, and is one
      way to measure the performance of an algorithm."
    textc:
      "Accuracy isn’t very sophisticated, but it will be good enough for now.
      We’ll dive into measuring algorithm performance more deeply in future
      lessons."

faq:
  heading: "Still have questions?"
  intro:
    "Check out the FAQ to the right!
    <br><br>
    Make sure you understand everything,
    because there will be a quiz after this."
  q1: "What if we had more than 2 classes?"
  a1: "<br>NEEDS ANSWER!<hr>"
  q2: "How would you write the code to make a KNN model?"
  a2: "<br>NEEDS ANSWER!<hr>"
  q3: "What if we had more than 2 predictors?"
  a3: "<br>NEEDS ANSWER!<hr>"
  q4: "What if set K = size of the entire dataset?"
  a4: "<br>NEEDS ANSWER!<hr>"
