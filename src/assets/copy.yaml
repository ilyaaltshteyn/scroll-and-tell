intro:
  lesson_num: "Lesson 1"
  lesson_title: "K-Nearest Neighbors Part 1: The Theory"
  preamble: "This is a theory lesson. You'll learn:"
  goals:
    - "We'll work with a dataset of Airbnb listings from Berlin."
    - "I'll introduce you to one kind of prediction problem that computers solve, called a <em>classification task</em>."
    - "We'll use the K-Nearest Neighbors (KNN) machine learning algorithm to solve
      a classification task in our Airbnb data."
    - "You'll tune our KNN model's <span class='tooltip'>hyperparameters<span class='tooltiptext'>This is just a fancy word for a model's settings.</span></span>."
    - "We'll measure how well our model performed on the prediction task."
part1:
  step1:
    heading: "The Dataset"
    texta:
      "For this lesson, we’ll use <span class='tooltip'>this dataset<a class='tooltiptext' href='google.com'>Link to dataset</a></span> of property sales. It has one row per
      property sold, and columns with some information about each property."
    textb:
      "Take a moment to explore the dataset."
  step2:
    heading: "Classification"
    texta:
      "A common task in machine learning is to train a computer to accurately
      guess whether a new mystery datapoint belongs in one group (also called a
      <em>class</em>) or another."
    textb:
      "In our dataset, each row is either a house or an apartment.
      These 2 groups (house & apartment) will be our classes."
  step3:
    heading: "Mystery Values"
    texta:
      "This data comes from a popular real estate listings website. Sometimes, the
      website’s users don’t fill out the 'property type' field, so there are some
      rows that have a missing value in that column."
    textb:
      "How could we train a computer to predict whether a new row is a house or an
      apartment?"
  step4:
    heading: "Classifying Mystery Values"
    texta:
      "The task above is asking us to train a computer to determine the class of a
      new row. This kind of task is called a classification task. We will solve it
      by using a classification algorithm."
    textb:
      "But first, let’s take a step back and graph this data. We’ll create a
      scatterplot, with a horizontal axis representing the property size, and a
      vertical axis representing the property price."
    textc:
      "We will draw datapoints representing entire homes in red, and ones
      representing private rooms in blue. If you forget, just mouse over any
      datapoint to see its class. Let's also include a few more datapoints from
      the same dataset, to make this more interesting."

part2:
  step1:
    heading:
      "Scatterplot!"
    texta:
      "Holy dog, a scatterplot! Now that we have the data plotted, let’s
      introduce the K-Nearest Neighbors algorithm to solve our classification task."
  step2:
    heading: "Mystery Datapoint"
    texta:
      "Let’s say we have this new datapoint— a property that sold for $XXX
      thousand, and has 720 square feet of space."
    textb:
      "The K-Nearest Neighbors algorithm will look at the nearest neighbors of
      this new datapoint, and will tally up how many of them are houses, and how
      many are apartments."
  step3:
    heading: "K Neighbors"
    texta: "How many neighbors will KNN look at?"
    textb:
      "That’s what K stands for! It’s the number of nearest neighbors to tally up!
      Let’s say that K is 5 for now."
    textc:
      "How does KNN know which datapoints are the nearest neighbors?"
    textd:
      "It calculates the distance from the new datapoint, (at location $XXX k, 5)
      to every other datapoint on the grid. Then it chooses the top 5."
  step4:
    heading: "Majority Wins"
    texta:
      "Then it will take the majority vote of those 5 datapoints, and that is the
      algorithm’s guess about what the mysterious new datapoint is."
    textb:
      "In this case, 4/5 of the nearest neighbors are apartments— so our best
      guess is that this new datapoint is an apartment."
  step5:
    heading: "What Happens if we Change K?"
    texta:
      "K is the model hyper-parameter that determines how many neighbors to take
      into account. We get to pick what K is. With two possible classes in our
      dataset, we can choose K to be any odd number, from 1 up to the size of the
      entire dataset."
    textb:
      "Use the slider below to play with the value for K. Notice how it affects
      the model’s guess about the class of the mystery datapoint.
      This is called tuning the model parameters, and is a big part of creating
      accurate machine learning models."
  step6:
    heading: "What a Simple Model!"
    texta:
      "We'll dive deeper into model parameter tuning in future lessons.
      For now, let’s just assume that we chose to use K = 5 for our model."
    textb:
      "When you first learn about KNN, it might seem wild that this could
      possibly work. It seems too basic, too reductive."
    textc:
      "How well does the model actually work? And how do we measure that?"
  step7:
    heading: "Testing our Model's Accuracy"
    texta:
      "Stopping here for now."

faq:
  q1: "What if we had more than 2 classes?"
  a1: "..."
  q2: "How would you actually make one of these models?"
