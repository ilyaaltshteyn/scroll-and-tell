intro:
  lesson_num: "Lesson 1"
  lesson_title: "The K-Nearest Neighbors Algorithm"
  preamble: "In this lesson:"
  goals:
    - "Identify classification problems"
    - "Use the K-Nearest Neighbors (KNN) machine learning algorithm to solve
    classification problems"
    - "Tune models by adjusting hyperparameters"
    - "Measure how well your model performed using its accuracy"
part1:
  step1:
    heading: "The Dataset"
    texta:
      "For this lesson, we’ll use this dataset of property sales. It has one row per
      property sold, and columns with some information about each property."
    textb:
      "Take a moment to explore the dataset."
  step2:
    heading: "Classification"
    texta:
      "A common task in machine learning is to train a computer to distinguish
      between different groups, or classes."
    textb:
      "In our dataset, each row is either a house or an apartment.
      These 2 groups (house & apartment) will be our classes."
  step3:
    heading: "Mystery Values"
    texta:
      "This data comes from a popular real estate listings website. Sometimes, the
      website’s users don’t fill out the 'property type' field, so there are some
      rows that have a missing value in that column."
    textb:
      "How could we train a computer to predict whether a new row is a house or an
      apartment?"
  step4:
    heading: "Classifying Mystery Values"
    texta:
      "The task above is asking us to train a computer to determine the class of a
      new row. This kind of task is called a classification task. We will solve it
      by using a classification algorithm."
    textb:
      "But first, let’s take a step back and graph this data. We’ll create a
      scatterplot, with a horizontal axis representing the property size, and a
      vertical axis representing the property price."
    textc:
      "We will draw datapoints representing entire homes in red, and ones
      representing private rooms in blue. If you forget, just mouse over any
      datapoint to see its class. Let's also include a few more datapoints from
      the same dataset, to make this more interesting."

part2:
  step1:
    heading:
      "Beautiful scatterplot"
    texta:
      "What a beautiful scatterplot! Now that we have the data plotted, let’s
      introduce the K-Nearest Neighbors algorithm to solve our classification task."
  step2:
    heading: "Mystery Datapoint"
    texta:
      "Let’s say we have this new datapoint— a property that sold for $XXX
      thousand, and has 720 square feet of space."
    textb:
      "The K-Nearest Neighbors algorithm will look at the nearest neighbors of
      this new datapoint, and will tally up how many of them are houses, and how
      many are apartments."
  step3:
    heading: "K Neighbors"
    texta: "How many neighbors will KNN look at?"
    textb:
      "That’s what K stands for! It’s the number of nearest neighbors to tally up!
      Let’s say that K is 5 for now."
    textc:
      "How does KNN know which datapoints are the nearest neighbors?"
    textd:
      "It calculates the distance from the new datapoint, (at location $XXX k, 5)
      to every other datapoint on the grid. Then it chooses the top 5."
  step4:
    heading: "Majority Wins"
    texta:
      "Then it will take the majority vote of those 5 datapoints, and that is the
      algorithm’s guess about what the mysterious new datapoint is."
    textb:
      "In this case, 4/5 of the nearest neighbors are apartments— so our best
      guess is that this new datapoint is an apartment."
