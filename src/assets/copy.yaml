name: "A Modern Scroll"
intro:
  lesson_num: "Lesson 1"
  lesson_title: "The K-Nearest Neighbors Algorithm, and Rookies
  <br>
  in the NBA."
  preamble1: "Pre-reqs:"
  goals1:
    - "An interest in machine learning."
    - "12 minutes of focused time."
  preamble2: "This lesson:"
  goals2:
    - "I'll introduce you to a fun dataset about rookie (1st year) NBA players."
    - "We'll use the K-Nearest Neighbors (KNN) algorithm to predict which rookies will have NBA careers that last 5 years or more."
    - "You will tune our KNN model's <span class='tooltip'>hyperparameters<span class='tooltiptext'>This is just a fancy word for a model's settings.</span></span>."
    - "We'll measure how accurately the model predicts which rookies will have lasting NBA careers."
part1:
  step1:
    heading: "The Dataset"
    texta:
      "The NBA records every action that happens on a basketball 
      court. For this lesson, we’ll use <span class='tooltip'>this awesome dataset<a class='tooltiptext' href='google.com'>Link to dataset</a></span> 
      of such records about 1st-year NBA players <em>(rookies)</em>.
      <br><br>
      It can be hard to know ahead of time which rookies will keep playing in 
      the NBA for while, and which will drop out to follow some different path.
      <br><br>
      We're going to use this dataset to create a simple model that will let us
      predict which rookie players will have careers that last at least 5 years. We'll
      be able to make this prediction about a player's future after they're done playing
      their rookie year in the NBA.
      <br><br>
      Our model will use just 2 datapoints about each player as inputs: (1) the number of shots
      that player made in their rookie year, and (2) the number of rebounds they had in that year.
      "
    textb:
      "Take a moment to explore the dataset. Mouse over column names for more info."
  step2:
    heading: "Classification"
    texta:
      "A common task in machine learning is to train a computer to accurately
      guess whether a new mystery datapoint belongs in one group (also called a
      <em>class</em>) or another."
    textb:
      "In our dataset, each row is either a house or an apartment.
      These 2 groups (house & apartment) will be our classes."
  step3:
    heading: "Mystery Values"
    texta:
      "This data comes from a popular real estate listings website. Sometimes, the
      website’s users don’t fill out the 'property type' field, so there are some
      rows that have a missing value in that column."
    textb:
      "How could we train a computer to predict whether a new row is a house or an
      apartment?"
  step4:
    heading: "Classifying Mystery Values"
    texta:
      "The task above is asking us to train a computer to determine the class of a
      new row. This kind of task is called a classification task. We will solve it
      by using a classification algorithm."
    textb:
      "But first, let’s take a step back and graph this data. We’ll create a
      scatterplot, with a horizontal axis representing the property size, and a
      vertical axis representing the property price."
    textc:
      "We will draw datapoints representing entire homes in red, and ones
      representing private rooms in blue. If you forget, just mouse over any
      datapoint to see its class. Let's also include a few more datapoints from
      the same dataset, to make this more interesting."

part2:
  step1:
    heading:
      "Scatterplot!"
    texta:
      "Holy dog, a scatterplot! Now that we have the data plotted, let’s
      introduce the K-Nearest Neighbors algorithm to solve our classification task."
  step2:
    heading: "Mystery Datapoint"
    texta:
      "Let’s say we have this new datapoint— a property that sold for $XXX
      thousand, and has 720 square feet of space."
    textb:
      "The K-Nearest Neighbors algorithm will look at the nearest neighbors of
      this new datapoint, and will tally up how many of them are houses, and how
      many are apartments."
  step3:
    heading: "K Neighbors"
    texta: "How many neighbors will KNN look at?"
    textb:
      "That’s what K stands for! It’s the number of nearest neighbors to tally up!
      Let’s say that K is 5 for now."
    textc:
      "How does KNN know which datapoints are the nearest neighbors?"
    textd:
      "It calculates the 
      <span class='tooltip'>
        distance
        <span class='tooltiptext'>
        A common way to calculate distance is using 
          <a href='https://en.wikipedia.org/wiki/Euclidean_distance' target='_blank'>
            the euclidean distance formula
          </a> you might remember from high school algebra.
        </span>
      </span> from the new datapoint, (at location $XXX k, 5)
      to every other datapoint on the grid. Then it chooses the top 5."
  step4:
    heading: "Majority Wins"
    texta:
      "Then it will take the majority vote of those 5 datapoints, and that is the
      algorithm’s guess about what the mysterious new datapoint is."
    textb:
      "In this case, 4/5 of the nearest neighbors are apartments— so our best
      guess is that this new datapoint is an apartment."
  step5:
    heading: "What Happens if we Change K?"
    texta:
      "K is the model hyper-parameter that determines how many neighbors to take
      into account. We get to pick what K is. With two possible classes in our
      dataset, we can choose K to be any odd number, from 1 up to the size of the
      entire dataset."
    textb:
      "Use the slider below to play with the value for K. Notice how it affects
      the model’s guess about the class of the mystery datapoint.
      This is called tuning the model parameters, and is a big part of creating
      accurate machine learning models."
  step6:
    heading: "What a Simple Model!"
    texta:
      "We'll dive deeper into model parameter tuning in future lessons.
      For now, let’s just assume that we chose to use K = 5 for our model."
    textb:
      "When you first learn about KNN, it might seem wild that this could
      possibly work. It seems too basic, too reductive."
    textc:
      "How well does the model actually work? And how do we measure that?"
  step7:
    heading: "Testing our Model's Accuracy"
    texta:
      "Let’s remove the labels (house/apartment) from 100 of our 500 datapoints,
      and run the algorithm on each of those 100 “mystery” datapoints. We know
      what their true labels are supposed to be, and we have KNN’s guesses about
      what the labels should be. So, we can count how many it got right."
    textb:
      "After doing this, we find that it guesses correctly on 75% of the
      datapoints! You would expect it to get 50% of them correct just by chance,
      so it’s way better than chance. This metric is called accuracy, and is one
      way to measure the performance of an algorithm."
    textc:
      "Accuracy isn’t very sophisticated, but it will be good enough for now.
      We’ll dive into measuring algorithm performance more deeply in future
      lessons."

faq:
  heading: "Still have questions?"
  intro:
    "Check out the FAQ to the right!
    <br><br>
    Make sure you understand everything,
    because there will be a quiz after this."
  q1: "What if we had more than 2 classes?"
  a1: "<br>NEEDS ANSWER!<hr>"
  q2: "How would you write the code to make a KNN model?"
  a2: "<br>NEEDS ANSWER!<hr>"
  q3: "What if we had more than 2 predictors?"
  a3: "<br>NEEDS ANSWER!<hr>"
  q4: "What if set K = size of the entire dataset?"
  a4: "<br>NEEDS ANSWER!<hr>"
